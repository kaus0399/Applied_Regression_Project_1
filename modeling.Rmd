---
title: "STATS402 Project1 Attempt:Google Analytics"
author: "Naiyu Niu, Kaustubh Deshpande, Jonathan Shan, Stephanie Lao"
date: "10/19/2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# EDA
## a). Histograms of Numerical Variables
```{r}
library(car)

data <- read.csv("final_data.csv")
dt = sort(sample(nrow(data), nrow(data)*.70))
train<-data[dt,]
test<-data[-dt,]
sapply(train, class)
```
```{r}
hist(train$summedRevenue, 
     main = "Histogram of Original Data", xlab = "summedRevenue_log")

hist(log(train$summedRevenue_log), 
     main = "Histogram of Log-Transformed Data",
     xlab = "log of summedRevenue_log")
```

From the above plots, we can see that the original response variable is hard to analyze with a gigantic peak at the first bar. After we perform log-transformation, the distribution of the response variable resembles a normal distribution, where we could apply (generalized) linear models. 




# Models
## a). Full Model
```{r}
full_mod <- lm(summedRevenue_log ~ avgGDP + factor(device.browser) + factor(device.operatingSystem) 
               + factor(device.deviceCategory) + factor(geoNetwork.continent) + factor(geoNetwork.subContinent) 
               + factor(geoNetwork.country) + totals.hits + totals.pageviews + factor(trafficSource.source),
               data = train)

summary(full_mod)

```


## b). Step Model
- Next we will obtain our Step Model which is chosen by R's `step()` function. This is the model with the least AIC.

```{r}
set.seed(1)
step_mod <- step(full_mod, direction = "both")
```
```{r}
plot(step_mod, which = 1)
plot(step_mod, which = 2)

residualPlots(step_mod)
```


## c). Manual Model
- Next we will attempt a manual implementations of models by intuitively handpicking the variables. 

```{r}
# Only categorical variables
cat_mod <- aov(summedRevenue_log ~ device.browser + device.operatingSystem 
               + device.deviceCategory + geoNetwork.continent + geoNetwork.country 
               + trafficSource.source,
               data = train)
summary(cat_mod)

# Only numerical variables
num_mod <- lm(summedRevenue_log ~ avgGDP + totals.hits + totals.pageviews,
              data = train)

summary(num_mod)
```


```{r}
# see if device.deviceCategory is needed
try1 <- aov(summedRevenue_log ~ device.browser + device.operatingSystem
            + geoNetwork.continent + geoNetwork.country 
            + trafficSource.source,
            data = train)
try2 <- aov(summedRevenue_log ~ device.browser + device.operatingSystem 
            + device.deviceCategory + geoNetwork.continent + geoNetwork.country 
            + trafficSource.source,
            data = train)

anova(try1, try2)
```


The result shows a non-significant result (p = 0.0859). Thus, we should reject model 2 and stick with model 1 which does not include device.deviceCategory


```{r}
manual_mod <- lm(summedRevenue_log ~ avgGDP + factor(device.browser) 
                 + factor(device.operatingSystem) + factor(geoNetwork.continent) 
                 + factor(geoNetwork.subContinent) + factor(geoNetwork.country) 
                 + totals.hits + totals.pageviews + factor(trafficSource.source),
                 data = train)

# check if manual_mod wil be rejected
anova(step_mod, manual_mod)
```

The result shows a non-significant result (p = 0.7531). Thus, we should reject the manual model and stick with our step model as **including the additional variables did not improve our fit**


## c). Weighted Least Squares Step Model
Since the step model has outperformed the manual model. It is currently our best. Let us analyze this model further. 

```{r}
plot(step_mod)
```

There may be slight heteroskedasticity present. Let's try and diagnose this


```{r}
#Let's find the Robust standard errors
library(lmtest)
library(sandwich)
summary(step_mod)
coeftest(step_mod, vcov = vcovHC(step_mod, "HC1"))
```


Robust standard errors are much lower than the standard errors. This can be attributed to small sample size of positive transaction data points.
Regressing using these robust standard errors will solve the issue of computing incorrect interval estimates or incorrect values for our test statistics. However, we still have another issue from heteroskedasticity, that the regular OLS estimators no longer being best. If we had a large enough sample size the variance of the estimators may still be small enough to get precise estimates. However, since our sample size is relatively small the variance is much larger and we need to conduct some further analysis. Let's try using weighted least squares. 


```{r}
#Let's try weighted least squares
library(MASS)
resids <- step_mod$residuals
varfunc.step_mod <- lm(abs(resids)~factor(device.browser) + factor(device.operatingSystem) + 
   + totals.hits, data = train)

weightz <- 1/((varfunc.step_mod$fitted.values)^2)

step_mod.wls = lm(summedRevenue_log~factor(device.browser) + factor(device.operatingSystem) + 
    factor(device.deviceCategory) + totals.hits + factor(trafficSource.source), weights = weightz, data = train)


summary(step_mod.wls)
# 
par(mfrow=c(1,2))
plot(step_mod, which = 1)
plot(step_mod.wls, which = 1)

par(mfrow=c(1,2))
plot(step_mod, which = 2)
plot(step_mod.wls, which = 2)

residualPlots(step_mod)
residualPlots(step_mod.wls)
```

```{r}

summary(step_mod)$r.squared
summary(step_mod.wls)$r.squared

#RMSE for step_mod
y_hat <- predict(step_mod,newdata = test)
residuals <- (test$summedRevenue_log - y_hat)
RMSE <- sqrt(mean(residuals^2))
RMSE

#RMSE for step_mod.wls 
y_hat <- predict(step_mod.wls,newdata = test)
residuals <- (test$summedRevenue_log - y_hat)
RMSE <- sqrt(mean(residuals^2))
RMSE
```

The weighted least squares step model seems to have a better R^2 value of 0.14 compared to the regualr step model's 0.12. Additionally the RMSE for the weighted least squares model is slightly less than the regular step model's rmse as well, 1.150 < 1.153.

Let us try leave one out cross validation, k-fold cross validation and anova to check for comparing our two mdoels and also checking for overfitting. 


# Comparing Final two models
## 10-fold Cross Validation
```{r}
library(ggplot2)

#Create vectors to stroe CV predcition errors
error_model_step <- rep(NA, 10)
error_model_step_wls <- rep(NA, 10)

for(i in 1:10){
   
   #Let us split the data between train and test.
   #70% will be used to train and 30% will be used to test. 
   dt = sort(sample(nrow(data), nrow(data)*.70))
   train<-data[dt,]
   test<-data[-dt,]
   
   step_mod = lm(summedRevenue_log~factor(device.browser) + factor(device.operatingSystem) + 
   + totals.hits, data = train)
   
   y_hat <- predict(step_mod,newdata = test)
   residuals <- (test$summedRevenue_log - y_hat)
   error_model_step[i] <- sqrt(mean(residuals^2))
   
   
   resids <- step_mod$residuals
   varfunc.step_mod <- lm(abs(resids)~factor(device.browser) + factor(device.operatingSystem) + 
     + totals.hits, data = train)

   
   weightz <- 1/((varfunc.step_mod$fitted.values)^2)
   

   step_mod.wls = lm(summedRevenue_log~factor(device.browser) + factor(device.operatingSystem) + 
   + totals.hits, weights = weightz, data = train)
   
   
   y_hat <- predict(step_mod.wls,newdata = test)
   residuals <- (test$summedRevenue_log - y_hat)
   error_model_step_wls[i] <- sqrt(mean(residuals^2))
  
}

mean(error_model_step)
mean(error_model_step_wls)
```


The low CV prediction errors confirm that neither of our models are overfitting as they can predict well on data the models have not been trained on. That being said, the regular step model seems to perform better than the WLS model as 1.64 < 1.67. Let's try Leave one out cross validation or PRESS to further test whether the regular step model is superior. 


## Leave one out Cross Validation
```{r}

# create vectors to store the validation errors for each model
# error_model1 <- rep(NA, 53)
# ...
error_model_step <- rep(NA, 1335)
error_model_step_wls <- rep(NA, 1335)

for(i in 1:1335){
  
  # write a line to select the ith line in the data
  # store this line as the 'test' case
  test <- data[i,]
  # store the remaining as the 'training' data
  train <- data[-c(i),]
  
  step_mod = lm(summedRevenue_log~factor(device.browser) + factor(device.operatingSystem) + totals.hits, data = train)
  
  y_hat <- predict(step_mod,newdata = test)
  residuals <- (test$summedRevenue_log - y_hat)
  error_model_step[i] <- sqrt(mean(residuals^2))
  
  resids <- step_mod$residuals
  varfunc.step_mod <- lm(abs(resids)~factor(device.browser) + factor(device.operatingSystem) + totals.hits, data = train)
  
  weightz <- 1/((varfunc.step_mod$fitted.values)^2)
  
  step_mod.wls = lm(summedRevenue_log~factor(device.browser) + factor(device.operatingSystem) + 
    + totals.hits, weights = weightz, data = train)
  
  y_hat <- predict(step_mod.wls,newdata = test)
  residuals <- (test$summedRevenue_log - y_hat)
  error_model_step_wls[i] <- sqrt(mean(residuals^2))
  
 
}


# once all of the errors have been calculated, find the mean squared error
# ...
mean(error_model_step)
mean(error_model_step_wls)
```


The LOOCV validation prediction error demonstrates that the WLS model is superior. However, LOOCV has a high variability as only one-observation validation-set is used for prediction. This makes the models generated prone to over fitting. As such we will proceed with the results of K-fold CV and **conclude that the regular step_model is our final selection.**


Now that we have selected our model let's test for some assumptions. Let us start off with multicollinearity. We will use the variance inflation factor or VIF to test this. 
```{r}
vif(step_mod)
```

Since our VIF scores are quite low for all of our variables we can conclude that no multicollinearity is present. 

**INSERT CHECKING ASSUMPTIONS CODE HERE**

